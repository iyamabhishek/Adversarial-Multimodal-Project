{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "import random\n",
    "import torch\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable, grad\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import h5py\n",
    "import time\n",
    "from collections import defaultdict, OrderedDict\n",
    "import argparse\n",
    "import cPickle as pickle\n",
    "import time\n",
    "import json, os, ast, h5py\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFN(nn.Module):\n",
    "    def __init__(self,config,NN1Config,NN2Config,gamma1Config,gamma2Config,outConfig):\n",
    "        super(MFN, self).__init__()\n",
    "        [self.d_m1] = config[\"input_dims\"]\n",
    "        [self.dh_m1] = config[\"h_dims\"]\n",
    "        total_h_dim = self.dh_m1\n",
    "        self.mem_dim = config[\"memsize\"]\n",
    "        window_dim = config[\"windowsize\"]\n",
    "        output_dim = 1\n",
    "        attInShape = total_h_dim*window_dim\n",
    "        gammaInShape = attInShape+self.mem_dim\n",
    "        final_out = total_h_dim+self.mem_dim\n",
    "        h_att1 = NN1Config[\"shapes\"]\n",
    "        h_att2 = NN2Config[\"shapes\"]\n",
    "        h_gamma1 = gamma1Config[\"shapes\"]\n",
    "        h_gamma2 = gamma2Config[\"shapes\"]\n",
    "        h_out = outConfig[\"shapes\"]\n",
    "        att1_dropout = NN1Config[\"drop\"]\n",
    "        att2_dropout = NN2Config[\"drop\"]\n",
    "        gamma1_dropout = gamma1Config[\"drop\"]\n",
    "        gamma2_dropout = gamma2Config[\"drop\"]\n",
    "        out_dropout = outConfig[\"drop\"]\n",
    "\n",
    "        self.lstm_m1 = nn.LSTMCell(self.d_m1, self.dh_m1)\n",
    "\n",
    "        self.att1_fc1 = nn.Linear(attInShape, h_att1)\n",
    "        self.att1_fc2 = nn.Linear(h_att1, attInShape)\n",
    "        self.att1_dropout = nn.Dropout(att1_dropout)\n",
    "\n",
    "        self.att2_fc1 = nn.Linear(attInShape, h_att2)\n",
    "        self.att2_fc2 = nn.Linear(h_att2, self.mem_dim)\n",
    "        self.att2_dropout = nn.Dropout(att2_dropout)\n",
    "\n",
    "        self.gamma1_fc1 = nn.Linear(gammaInShape, h_gamma1)\n",
    "        self.gamma1_fc2 = nn.Linear(h_gamma1, self.mem_dim)\n",
    "        self.gamma1_dropout = nn.Dropout(gamma1_dropout)\n",
    "\n",
    "        self.gamma2_fc1 = nn.Linear(gammaInShape, h_gamma2)\n",
    "        self.gamma2_fc2 = nn.Linear(h_gamma2, self.mem_dim)\n",
    "        self.gamma2_dropout = nn.Dropout(gamma2_dropout)\n",
    "\n",
    "        self.out_fc1 = nn.Linear(final_out, h_out)\n",
    "        self.out_fc2 = nn.Linear(h_out, output_dim)\n",
    "        self.out_dropout = nn.Dropout(out_dropout)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x_m1 = x[:,:,:self.d_m1]\n",
    "        # x is t x n x d\n",
    "        n = x.shape[1]\n",
    "        t = x.shape[0]\n",
    "        self.h_m1 = torch.zeros(n, self.dh_m1).cuda()\n",
    "        self.c_m1 = torch.zeros(n, self.dh_m1).cuda()\n",
    "        self.mem = torch.zeros(n, self.mem_dim).cuda()\n",
    "        all_h_m1s = []\n",
    "        all_c_m1s = []\n",
    "        all_mems = []\n",
    "        for i in range(t):\n",
    "            # prev time step\n",
    "            prev_c_m1 = self.c_m1\n",
    "            # curr time step\n",
    "            new_h_m1, new_c_m1 = self.lstm_m1(x_m1[i], (self.h_m1, self.c_m1))\n",
    "            # concatenate\n",
    "            prev_cs = prev_c_m1\n",
    "            new_cs = new_c_m1\n",
    "            cStar = torch.cat([prev_cs,new_cs], dim=1)\n",
    "            attention = F.softmax(self.att1_fc2(self.att1_dropout(F.relu(self.att1_fc1(cStar)))),dim=1)\n",
    "            attended = attention*cStar\n",
    "            cHat = F.tanh(self.att2_fc2(self.att2_dropout(F.relu(self.att2_fc1(attended)))))\n",
    "            both = torch.cat([attended,self.mem], dim=1)\n",
    "            gamma1 = F.sigmoid(self.gamma1_fc2(self.gamma1_dropout(F.relu(self.gamma1_fc1(both)))))\n",
    "            gamma2 = F.sigmoid(self.gamma2_fc2(self.gamma2_dropout(F.relu(self.gamma2_fc1(both)))))\n",
    "            self.mem = gamma1*self.mem + gamma2*cHat\n",
    "            all_mems.append(self.mem)\n",
    "            # update\n",
    "            self.h_m1, self.c_m1 = new_h_m1, new_c_m1\n",
    "            all_h_m1s.append(self.h_m1)\n",
    "            all_c_m1s.append(self.c_m1)\n",
    "\n",
    "        # last hidden layer last_hs is n x h\n",
    "        last_h_m1 = all_h_m1s[-1]\n",
    "        last_mem = all_mems[-1]\n",
    "        last_hs = torch.cat([last_h_m1,last_mem], dim=1)\n",
    "        output = self.out_fc2(self.out_dropout(F.relu(self.out_fc1(last_hs))))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mfn(X_train, y_train, X_valid, y_valid, X_test, y_test, configs):\n",
    "    p = np.random.permutation(X_train.shape[0])\n",
    "    X_train = X_train[p]\n",
    "    y_train = y_train[p]\n",
    "\n",
    "    X_train = X_train.swapaxes(0,1)\n",
    "    X_valid = X_valid.swapaxes(0,1)\n",
    "    X_test = X_test.swapaxes(0,1)\n",
    "\n",
    "    d = X_train.shape[2]\n",
    "    h = 128\n",
    "    t = X_train.shape[0]\n",
    "    output_dim = 1\n",
    "    dropout = 0.5\n",
    "\n",
    "    [config,NN1Config,NN2Config,gamma1Config,gamma2Config,outConfig] = configs\n",
    "\n",
    "    model = MFN(config,NN1Config,NN2Config,gamma1Config,gamma2Config,outConfig)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(),lr=config[\"lr\"])\n",
    "    \n",
    "    criterion = nn.L1Loss()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "    scheduler = ReduceLROnPlateau(optimizer,mode='min',patience=100,factor=0.5,verbose=True)\n",
    "\n",
    "    def train(model, batchsize, X_train, y_train, optimizer, criterion):\n",
    "        epoch_loss = 0\n",
    "        model.train()\n",
    "        total_n = X_train.shape[1]\n",
    "        num_batches = total_n / batchsize\n",
    "        for batch in xrange(num_batches):\n",
    "            start = batch*batchsize\n",
    "            end = (batch+1)*batchsize\n",
    "            optimizer.zero_grad()\n",
    "            batch_X = torch.Tensor(X_train[:,start:end]).cuda()\n",
    "            batch_y = torch.Tensor(y_train[start:end]).cuda()\n",
    "            predictions = model.forward(batch_X).squeeze(1)\n",
    "            loss = criterion(predictions, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        return epoch_loss / num_batches\n",
    "\n",
    "    def evaluate(model, X_valid, y_valid, criterion):\n",
    "        epoch_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            batch_X = torch.Tensor(X_valid).cuda()\n",
    "            batch_y = torch.Tensor(y_valid).cuda()\n",
    "            predictions = model.forward(batch_X).squeeze(1)\n",
    "            epoch_loss = criterion(predictions, batch_y).item()\n",
    "        return epoch_loss\n",
    "\n",
    "    def predict(model, X_test):\n",
    "        epoch_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            batch_X = torch.Tensor(X_test).cuda()\n",
    "            predictions = model.forward(batch_X).squeeze(1)\n",
    "            predictions = predictions.cpu().data.numpy()\n",
    "        return predictions\n",
    "\n",
    "    best_valid = 999999.0\n",
    "    rand = random.randint(0,100000)\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        train_loss = train(model, config[\"batchsize\"], X_train, y_train, optimizer, criterion)\n",
    "        valid_loss = evaluate(model, X_valid, y_valid, criterion)\n",
    "        scheduler.step(valid_loss)\n",
    "        if valid_loss <= best_valid:\n",
    "            # save model\n",
    "            best_valid = valid_loss\n",
    "            print epoch, train_loss, valid_loss, 'saving model'\n",
    "            torch.save(model, 'temp_models/mfn_unimodal.pt')\n",
    "        else:\n",
    "            print epoch, train_loss, valid_loss\n",
    "\n",
    "    print 'model number is:', rand\n",
    "    model = torch.load('temp_models/mfn_unimodal.pt')\n",
    "\n",
    "    predictions = predict(model, X_test)\n",
    "    mae = np.mean(np.absolute(predictions-y_test))\n",
    "    print \"mae: \", mae\n",
    "    corr = np.corrcoef(predictions,y_test)[0][1]\n",
    "    print \"corr: \", corr\n",
    "    mult = round(sum(np.round(predictions)==np.round(y_test))/float(len(y_test)),5)\n",
    "    print \"mult_acc: \", mult\n",
    "    f_score = round(f1_score(np.round(predictions),np.round(y_test),average='weighted'),5)\n",
    "    print \"mult f_score: \", f_score\n",
    "    true_label = (y_test >= 0)\n",
    "    predicted_label = (predictions >= 0)\n",
    "    print \"Confusion Matrix :\"\n",
    "    print confusion_matrix(true_label, predicted_label)\n",
    "    print \"Classification Report :\"\n",
    "    print classification_report(true_label, predicted_label, digits=5)\n",
    "    print \"Accuracy \", accuracy_score(true_label, predicted_label)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(X_test, y_test, metric):\n",
    "    X_test = X_test.swapaxes(0,1)\n",
    "    def predict(model, X_test):\n",
    "        epoch_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            batch_X = torch.Tensor(X_test).cuda()\n",
    "            predictions = model.forward(batch_X).squeeze(1)\n",
    "            predictions = predictions.cpu().data.numpy()\n",
    "        return predictions\n",
    "    if metric == 'mae':\n",
    "        model = torch.load('temp_models/mfn_unimodal.pt',map_location='cuda:0')\n",
    "    if metric == 'acc':\n",
    "        model = torch.load('temp_models/mfn_unimodal.pt',map_location='cuda:0')\n",
    "    model = model.cpu().cuda()\n",
    "    \n",
    "    predictions = predict(model, X_test)\n",
    "    print predictions.shape\n",
    "    print y_test.shape\n",
    "    mae = np.mean(np.absolute(predictions-y_test))\n",
    "    print \"mae: \", mae\n",
    "    corr = np.corrcoef(predictions,y_test)[0][1]\n",
    "    print \"corr: \", corr\n",
    "    mult = round(sum(np.round(predictions)==np.round(y_test))/float(len(y_test)),5)\n",
    "    print \"mult_acc: \", mult\n",
    "    f_score = round(f1_score(np.round(predictions),np.round(y_test),average='weighted'),5)\n",
    "    print \"mult f_score: \", f_score\n",
    "    true_label = (y_test >= 0)\n",
    "    predicted_label = (predictions >= 0)\n",
    "    print \"Confusion Matrix :\"\n",
    "    print confusion_matrix(true_label, predicted_label)\n",
    "    print \"Classification Report :\"\n",
    "    print classification_report(true_label, predicted_label, digits=5)\n",
    "    print \"Accuracy \", accuracy_score(true_label, predicted_label)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_saved_data(x_train, x_valid, x_test, y_train, y_valid, y_test):\n",
    "    h5f = h5py.File(x_train,'r')\n",
    "    X_train = h5f['data'][:]\n",
    "    h5f.close()\n",
    "    h5f = h5py.File(y_train,'r')\n",
    "    y_train = h5f['data'][:]\n",
    "    h5f.close()\n",
    "    h5f = h5py.File(x_valid,'r')\n",
    "    X_valid = h5f['data'][:]\n",
    "    h5f.close()\n",
    "    h5f = h5py.File(y_valid,'r')\n",
    "    y_valid = h5f['data'][:]\n",
    "    h5f.close()\n",
    "    h5f = h5py.File(x_test,'r')\n",
    "    X_test = h5f['data'][:]\n",
    "    h5f.close()\n",
    "    h5f = h5py.File(y_test,'r')\n",
    "    y_test = h5f['data'][:]\n",
    "    h5f.close()\n",
    "    return X_train, y_train, X_valid, y_valid, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_valid, y_valid, X_test, y_test = load_saved_data('data/x_unimodal_a_train.h5', 'data/x_unimodal_a_valid.h5', 'data/x_unimodal_a_test.h5', 'data/y_train.h5', 'data/y_valid.h5', 'data/y_test.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'input_dims': [5], 'batchsize': 128, 'memsize': 300, 'windowsize': 2, 'lr': 0.005, 'num_epochs': 50, 'h_dims': [32], 'momentum': 0.3}, {'shapes': 32, 'drop': 0.2}, {'shapes': 128, 'drop': 0.2}, {'shapes': 64, 'drop': 0.0}, {'shapes': 64, 'drop': 0.5}, {'shapes': 32, 'drop': 0.2}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhishek/anaconda3/envs/mfn/lib/python2.7/site-packages/torch/nn/functional.py:1320: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "/home/abhishek/anaconda3/envs/mfn/lib/python2.7/site-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.32199912071 1.41654348373 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhishek/anaconda3/envs/mfn/lib/python2.7/site-packages/torch/serialization.py:250: UserWarning: Couldn't retrieve source code for container of type MFN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1.31882865429 1.41455864906 saving model\n",
      "2 1.30847758055 1.40521609783 saving model\n",
      "3 1.30300242901 1.40683627129\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-ef5f38b99d89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mconfigs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mNN1Config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mNN2Config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgamma1Config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgamma2Config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutConfig\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mtrain_mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-4e0a6e714252>\u001b[0m in \u001b[0;36mtrain_mfn\u001b[0;34m(X_train, y_train, X_valid, y_valid, X_test, y_test, configs)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mrand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_epochs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"batchsize\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-4e0a6e714252>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, batchsize, X_train, y_train, optimizer, criterion)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/abhishek/anaconda3/envs/mfn/lib/python2.7/site-packages/torch/tensor.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m \u001b[0;34m`\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/abhishek/anaconda3/envs/mfn/lib/python2.7/site-packages/torch/autograd/__init__.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# config[\"input_dims\"] = [300,5,20]\n",
    "# hm1 = random.choice([32,64,88,128,156,256])\n",
    "# hm2 = random.choice([8,16,32,48,64,80])\n",
    "# hm3 = random.choice([8,16,32,48,64,80])\n",
    "# config[\"h_dims\"] = [hm1,hm2,hm3]\n",
    "\n",
    "config = dict()\n",
    "config[\"input_dims\"] = [5]\n",
    "hm1 = random.choice([32,64,88,128,156,256])\n",
    "config[\"h_dims\"] = [hm1]\n",
    "config[\"memsize\"] = random.choice([64,128,256,300,400])\n",
    "config[\"windowsize\"] = 2\n",
    "config[\"batchsize\"] = random.choice([32,64,128,256])\n",
    "config[\"num_epochs\"] = 50\n",
    "config[\"lr\"] = random.choice([0.001,0.002,0.005,0.008,0.01])\n",
    "config[\"momentum\"] = random.choice([0.1,0.3,0.5,0.6,0.8,0.9])\n",
    "NN1Config = dict()\n",
    "NN1Config[\"shapes\"] = random.choice([32,64,128,256])\n",
    "NN1Config[\"drop\"] = random.choice([0.0,0.2,0.5,0.7])\n",
    "NN2Config = dict()\n",
    "NN2Config[\"shapes\"] = random.choice([32,64,128,256])\n",
    "NN2Config[\"drop\"] = random.choice([0.0,0.2,0.5,0.7])\n",
    "gamma1Config = dict()\n",
    "gamma1Config[\"shapes\"] = random.choice([32,64,128,256])\n",
    "gamma1Config[\"drop\"] = random.choice([0.0,0.2,0.5,0.7])\n",
    "gamma2Config = dict()\n",
    "gamma2Config[\"shapes\"] = random.choice([32,64,128,256])\n",
    "gamma2Config[\"drop\"] = random.choice([0.0,0.2,0.5,0.7])\n",
    "outConfig = dict()\n",
    "outConfig[\"shapes\"] = random.choice([32,64,128,256])\n",
    "outConfig[\"drop\"] = random.choice([0.0,0.2,0.5,0.7])\n",
    "configs = [config,NN1Config,NN2Config,gamma1Config,gamma2Config,outConfig]\n",
    "print configs\n",
    "train_mfn(X_train, y_train, X_valid, y_valid, X_test, y_test, configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(686,)\n",
      "(686,)\n",
      "mae:  1.4551973826665359\n",
      "corr:  0.23423482563048087\n",
      "mult_acc:  0.1516\n",
      "mult f_score:  0.23861\n",
      "Confusion Matrix :\n",
      "[[  0 379]\n",
      " [  0 307]]\n",
      "Classification Report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False    0.00000   0.00000   0.00000       379\n",
      "        True    0.44752   1.00000   0.61833       307\n",
      "\n",
      "   micro avg    0.44752   0.44752   0.44752       686\n",
      "   macro avg    0.22376   0.50000   0.30916       686\n",
      "weighted avg    0.20028   0.44752   0.27672       686\n",
      "\n",
      "Accuracy  0.44752186588921283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhishek/anaconda3/envs/mfn/lib/python2.7/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/home/abhishek/anaconda3/envs/mfn/lib/python2.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "test(X_test, y_test, 'mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
